{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from collections import Counter\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = \"C://Users//shiba//Downloads//Naruto Project//final_model.keras\"\n",
    "JSON_MAPPING_PATH = \"C://Users//shiba//Downloads//Naruto Project//Jutsu_dataset.json\"  # Path to Jutsu mapping\n",
    "CLASS_INDICES_PATH = \"C://Users//shiba//Downloads//Naruto Project//class_indices.json\"  # Path to class indices\n",
    "IMG_SIZE = 224\n",
    "DETECTION_WINDOW_SIZE = 400  # Increased size of the detection window (400x400 pixels)\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Load the JSON mapping for Jutsus\n",
    "with open(JSON_MAPPING_PATH, 'r') as file:\n",
    "    jutsu_mapping = json.load(file)\n",
    "\n",
    "# Load class indices mapping\n",
    "with open(CLASS_INDICES_PATH, 'r') as file:\n",
    "    class_indices = json.load(file)\n",
    "\n",
    "# Reverse the class indices to map index back to class names\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Helper function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    frame_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    img_array = img_to_array(frame_resized) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Match the sequence to a Jutsu\n",
    "def match_sequence_to_jutsu(sequence, jutsu_mapping):\n",
    "    for entry in jutsu_mapping['data']:\n",
    "        if entry['sequence'] == sequence:\n",
    "            return entry['jutsu']\n",
    "    return \"Unknown Jutsu\"\n",
    "\n",
    "# Get the most frequent hand sign in the detection window\n",
    "def get_most_frequent(predictions):\n",
    "    if predictions:\n",
    "        return Counter(predictions).most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "# Check if the center of the hand sign is inside the detection window\n",
    "def is_hand_in_window(x, y, frame_width, frame_height, window_size=DETECTION_WINDOW_SIZE):\n",
    "    # Calculate the center of the frame\n",
    "    center_x = frame_width // 2\n",
    "    center_y = frame_height // 2\n",
    "    # Define the window boundaries\n",
    "    window_left = center_x - window_size // 2\n",
    "    window_top = center_y - window_size // 2\n",
    "    window_right = center_x + window_size // 2\n",
    "    window_bottom = center_y + window_size // 2\n",
    "    # Check if the hand is within the window\n",
    "    return window_left <= x <= window_right and window_top <= y <= window_bottom\n",
    "\n",
    "# Process video and detect Jutsu\n",
    "def process_video(input_video_path, output_video_path, fps_window=2):\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frames_per_window = fps * fps_window  # Number of frames in the 1-second window\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    \n",
    "    # Define output video writer\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    predicted_sequence = []  # Final sequence of hand signs\n",
    "    window_predictions = []  # Store predictions within the current window\n",
    "    frame_count = 0  # Track frames within the window\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame and make predictions\n",
    "        img_array = preprocess_frame(frame)\n",
    "        prediction = model.predict(img_array)\n",
    "        predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "        predicted_class = index_to_class[predicted_class_index]\n",
    "        \n",
    "        # Get the center of the hand sign (simulated here with random values; replace with real hand detection)\n",
    "        # In real application, use hand tracking (e.g., MediaPipe or OpenCV hand tracking)\n",
    "        # This step should provide the (x, y) coordinates of the hand center.\n",
    "        hand_x, hand_y = width // 2, height // 2  # Simulating the hand in the center (replace with actual hand detection)\n",
    "        \n",
    "        # If the hand is inside the detection window, collect predictions\n",
    "        if is_hand_in_window(hand_x, hand_y, width, height):\n",
    "            window_predictions.append(predicted_class)\n",
    "            frame_count += 1\n",
    "        \n",
    "        # If the window is full, determine the most frequent hand sign\n",
    "        if frame_count >= frames_per_window:\n",
    "            most_frequent_sign = get_most_frequent(window_predictions)\n",
    "            if most_frequent_sign and (not predicted_sequence or most_frequent_sign != predicted_sequence[-1]):\n",
    "                predicted_sequence.append(most_frequent_sign)\n",
    "            \n",
    "            # Clear the window for the next segment\n",
    "            window_predictions = []\n",
    "            frame_count = 0\n",
    "        \n",
    "        # Match the current sequence to a Jutsu\n",
    "        detected_jutsu = match_sequence_to_jutsu(predicted_sequence, jutsu_mapping)\n",
    "        \n",
    "        # Display predictions on the frame\n",
    "        cv2.putText(frame, f\"Hand Sign: {predicted_class}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Sequence: {predicted_sequence}\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Jutsu: {detected_jutsu}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw the detection window on the frame\n",
    "        center_x = width // 2\n",
    "        center_y = height // 2\n",
    "        cv2.rectangle(frame, (center_x - DETECTION_WINDOW_SIZE // 2, center_y - DETECTION_WINDOW_SIZE // 2),\n",
    "                      (center_x + DETECTION_WINDOW_SIZE // 2, center_y + DETECTION_WINDOW_SIZE // 2),\n",
    "                      (0, 255, 0), 2)\n",
    "        \n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_video_path = \"C://Users//shiba//Downloads//Naruto Project//input.mp4\"  # Replace with your input video path\n",
    "output_video_path = \"C://Users//shiba//Downloads//Naruto Project//output_video.avi\"  # Replace with your desired output video path\n",
    "\n",
    "process_video(input_video_path, output_video_path)\n",
    "print(\"Video processing complete. Output saved at:\", output_video_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
